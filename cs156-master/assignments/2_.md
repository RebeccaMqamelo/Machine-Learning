# Assignment 2

## Assignment instructions:

### Lending Club Data

The Lending Club has made an anonymized set of data available for anyone to study [here](https://www.lendingclub.com/info/download-data.action) or if you do not have a lending club account the data is also available on Kaggle [here](https://www.kaggle.com/wordsforthewise/lending-club). Descriptions of the columns can be found [here](https://www.kaggle.com/wendykan/lending-club-loan-data#LCDataDictionary.xlsx) under 'LoanStats' and 'RejectStats'.

The Lending Club is a platform which allows the crowdfunding of various loans. Various investors are able to browse the profiles of people applying for loans and decide whether or not to help fun them.

In this assignment you will build a model that predicts the largest loan amount that will be successfully funded for any given individual. This model can then be used to advise the applicants on how much they could apply for.

#### Detailed Instructions

##### Tools 

Jupyter Notebook, pandas, sklearn



##### Data Cleaning

- You'll need to download both reject data and approved data, and combine them into one dataset. You likely won't need all available data; make a smart decision about how many `csv` files to download and use (and feel free to use a mere slice when initially creating your model, if you find that training takes a while, but once you have working code you should train your model on the larger dataset). *This means making the sets uniform somehow, combining them, and splitting them into a training set and holdout set.*
- Load in your `csv`s with pandas' `read_csv`. Good habit: check out what's going on in your dataframe (e.g `df`) by calling `df.head()`.
- As you'll see, the columns in the reject data and approved data are very different. Use the spreadsheet available on the download page to figure out which of the variables are present in both sets (by different names)
  - You'll have to drop columns -- definitely from the approved data, and maybe from the reject data
- There may be a lot of missing data. You'll have to make decisions about this: 
  - Do any columns have so much missing data so as not to be useful? (Hint: [pandas has ways](https://pandas.pydata.org/pandas-docs/stable/missing_data.html) to check for missing values by column, or even in the entire dataframe) 
  - Do any have only a few missing values? You'll have to decide whether/where to impute, based on whether it's feasible and whether the values might be important, given the problem.
- Consider whether scaling will be necessary for any of your variables (For a very basic introduction to feature scaling, see [this chapter](https://classroom.udacity.com/courses/ud120/lessons/2864738562/concepts/29983887730923) from Udacity's course on sklearn. It says it will take 2 hours; it will really take 30 minutes max. But, if pressed for time, do this after you get a working model).
- Identify your variable types -- one of categorical, numerical, or ordinal -- and encode them accordingly with a numeric representation (e.g., one-hot encoding, factorizing). Make appropriate choices. Consider sklearn's `LabelBinarizer`, `LabelEncoder`, etc...
- Append your new compatible, uniform datasets together.
- (*optional*) If your approach to this problem entails predicting for accept/reject status, then you may need to add a column indicating which rows come from the reject set, and which from the approved set.
- Use sklearn's `train_test_split` to get a training set and holdout set, making sure to split each class into the two sets.



##### Modeling & Evaluation

- Note: as is the case for any ML problem, while you make decisions about your model's hyperparameters (i.e., assuming you are using sklearn's `LinearRegression` algorithm: [all of the parameters](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) you can pass to the model before fitting -- though in general, classifier algorithms have way more of these), or while you make decisions about how to normalize/rescale your data, you'll want to get a sense of how your model performs on unseen data. Don't use the holdout set! If you're testing on your holdout set in order to guide new decisions, your model with suffer from [information leakage](https://machinelearningmastery.com/data-leakage-machine-learning/), roughly meaning that your final evaluation on the test set won't really indicate how well the model actually performs on unseen data.
  - You'll have to decide how to use cross-validation during training. The sklearn library offers a [number of ways](http://scikit-learn.org/stable/modules/cross_validation.html) to do this.
- Once you decide on a model, consider how best to evaluate it: what is it's error rate? Which metric do you use?
- Your actual approach to the problem is up to you: what value to predict for, how to arrive at a decision boundary, etc...



##### To Hand In

A clear report explaining:

- which variables you included in the model
- any cleaning or transformations that you carried out on the data
- the type of model you used and any settings that the model required
- the training method you used, and any techniques that you used to avoid overfitting the data
- an estimate of how well the model will perform on unseen data

The code for:

- cleaning and transforming the data (if any)
- fitting the model
- evaluating its performance

If there are any issues or concerns with your model, or the data that you trained on, be sure to highlight this in your report. An example of such a concern might be that the maximum funded amount might vary over time, and how that might affect your model, or your selection of data. Since this is real-world data there are bound to be issues of this sort!

For this assignment, **methodological correctness is the most important aspect that will be examined**. It is more important to do things correctly than to build a highly accurate model. Once the course has covered more models, then accuracy will become the focus of the assignments, and methodological correctness will be taken for granted.

Please convert everything to a single PDF file and submit it. 

---

